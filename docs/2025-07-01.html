<!DOCTYPE html>
<html lang="ja">
<!-- Gemini 2.5 Proによる生成です。 -->
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>機械学習の活性化関数一覧</title>
    
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" xintegrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    
    <!-- MathJax for rendering LaTeX formulas -->
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>

    <!-- Chart.js for graphs -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.2/dist/chart.umd.min.js"></script>

    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;500;700&display=swap');
        body {
            font-family: 'Noto Sans JP', sans-serif;
            background-color: #f8f9fa;
        }
        .table th {
            position: sticky;
            top: 0;
            background-color: #e9ecef;
        }
        .formula {
            font-family: monospace;
            font-size: 1.1rem;
            color: #495057;
        }
        .table-responsive {
            max-height: 90vh;
        }
    </style>
</head>
<body>

    <div class="container py-5">
        <header class="text-center mb-5">
            <h1 class="display-4 fw-bold">機械学習の活性化関数</h1>
            <p class="lead text-muted">ニューラルネットワークで頻繁に利用される活性化関数の一覧です。</p>
        </header>

        <div class="table-responsive shadow-sm rounded">
            <table class="table table-hover table-bordered align-middle bg-white mb-0">
                <thead class="table-light">
                    <tr class="text-center">
                        <th class="p-3">関数名</th>
                        <th class="p-3">グラフ</th>
                        <th class="p-3">数式</th>
                        <th class="p-3">特徴</th>
                        <th class="p-3">主な用途</th>
                    </tr>
                </thead>
                <tbody>
                    <!-- Sigmoid Function -->
                    <tr>
                        <td class="fw-medium p-3">シグモイド関数</td>
                        <td class="p-3" style="min-width: 200px;"><canvas id="sigmoidChart"></canvas></td>
                        <td class="p-3 formula">$\sigma(x) = \frac{1}{1 + e^{-x}}$</td>
                        <td class="p-3">出力を0から1の間に押し込めるS字カーブを描きます。確率として解釈しやすく、古くから使われてきました。</td>
                        <td class="p-3">二値分類の出力層、確率の出力。</td>
                    </tr>

                    <!-- ReLU Function -->
                    <tr>
                        <td class="fw-medium p-3">ReLU関数</td>
                        <td class="p-3"><canvas id="reluChart"></canvas></td>
                        <td class="p-3 formula">$f(x) = \max(0, x)$</td>
                        <td class="p-3">計算が非常に高速で、勾配消失問題を起きにくくしたことで深層学習の発展を加速させました。現在の標準的な選択肢です。</td>
                        <td class="p-3">深層学習モデルの中間層</td>
                    </tr>

                    <!-- Leaky ReLU Function -->
                    <tr>
                        <td class="fw-medium p-3">Leaky ReLU関数</td>
                        <td class="p-3"><canvas id="leakyReluChart"></canvas></td>
                        <td class="p-3 formula">$f(x) = \begin{cases} x & \text{if } x \ge 0 \\ 0.01x & \text{if } x < 0 \end{cases}$</td>
                        <td class="p-3">ReLUの派生版。入力が負でもわずかな勾配を持つため、ニューロンが完全に「死んでしまう」のを防ぎます。</td>
                        <td class="p-3">ReLUで学習がうまくいかない場合の代替案</td>
                    </tr>
                    
                    <!-- Softmax Function -->
                    <tr>
                        <td class="fw-medium p-3">ソフトマックス関数</td>
                        <td class="p-3"><canvas id="softmaxChart"></canvas></td>
                        <td class="p-3 formula">$\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}$</td>
                        <td class="p-3">複数の出力の合計が1になるように正規化します。これにより、出力をカテゴリごとの「確率分布」として表現できます。（グラフは入力の一つを変化させた時の出力確率）</td>
                        <td class="p-3">多クラス分類の出力層</td>
                    </tr>

                    <!-- Softplus Function -->
                    <tr>
                        <td class="fw-medium p-3">Softplus関数</td>
                        <td class="p-3"><canvas id="softplusChart"></canvas></td>
                        <td class="p-3 formula">$f(x) = \log(1 + e^x)$</td>
                        <td class="p-3">ReLUを滑らかにしたような関数です。全体で微分可能であり、出力が常に正になるという性質を持ちます。</td>
                        <td class="p-3">ReLUの代替、出力が必ず正の値を取るようにしたい場合</td>
                    </tr>

                    <!-- Swish Function -->
                    <tr>
                        <td class="fw-medium p-3">Swish関数</td>
                        <td class="p-3"><canvas id="swishChart"></canvas></td>
                        <td class="p-3 formula">$f(x) = x \cdot \sigma(x)$</td>
                        <td class="p-3">Googleが提案した関数。ReLUのように非線形でありながら、負の値も許容する滑らかな曲線を描き、多くの場合でReLUより高い性能を示します。</td>
                        <td class="p-3">深層学習モデルの中間層（特に画像認識）</td>
                    </tr>

                    <!-- GELU Function -->
                    <tr>
                        <td class="fw-medium p-3">GELU関数</td>
                        <td class="p-3"><canvas id="geluChart"></canvas></td>
                        <td class="p-3 formula" style="font-size: 0.8rem;">$f(x) \approx 0.5x(1 + \tanh(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)))$</td>
                        <td class="p-3">確率的なアプローチを取り入れた関数で、入力の大きさに応じて活性化するかどうかを滑らかに決定します。自然言語処理で絶大な成果を上げています。</td>
                        <td class="p-3">Transformerベースのモデル（Gemini、GPTなど）の中間層</td>
                    </tr>
                </tbody>
            </table>
        </div>
    </div>

    <!-- Bootstrap JS Bundle -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" xintegrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz" crossorigin="anonymous"></script>

    <script>
    document.addEventListener('DOMContentLoaded', function () {
        // --- Chart Data and Configuration ---
        const labels = [];
        for (let i = -5; i <= 5; i += 0.25) {
            labels.push(i.toFixed(2));
        }

        const chartOptions = {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
                x: {
                    grid: { display: false },
                    ticks: {
                        maxTicksLimit: 5,
                        autoSkip: true,
                    }
                },
                y: { 
                    grid: { display: false },
                    ticks: {
                        maxTicksLimit: 5,
                        autoSkip: true,
                    }
                }
            },
            plugins: {
                legend: { display: false }
            },
            elements: {
                point: { radius: 0 },
                line: {
                    borderWidth: 2.5,
                    tension: 0.1
                }
            }
        };

        // --- Function Definitions ---
        const sigmoid = x => 1 / (1 + Math.exp(-x));
        const relu = x => Math.max(0, x);
        const leakyRelu = x => (x >= 0 ? x : 0.01 * x);
        const softplus = x => Math.log(1 + Math.exp(x));
        const swish = x => x * sigmoid(x);
        const gelu = x => 0.5 * x * (1 + Math.tanh(Math.sqrt(2 / Math.PI) * (x + 0.044715 * Math.pow(x, 3))));
        // For softmax, we show how one output changes when its input logit changes, assuming other logits are constant.
        const softmax = x => Math.exp(x) / (Math.exp(x) + Math.exp(1.0) + Math.exp(0.5)); // Other inputs are 1.0 and 0.5

        // --- Chart Creation ---
        function createChart(canvasId, dataFunction, color) {
            const ctx = document.getElementById(canvasId).getContext('2d');
            new Chart(ctx, {
                type: 'line',
                data: {
                    labels: labels,
                    datasets: [{
                        label: canvasId.replace('Chart', ''),
                        data: labels.map(x => dataFunction(parseFloat(x))),
                        borderColor: color,
                        backgroundColor: color,
                    }]
                },
                options: chartOptions
            });
        }

        createChart('sigmoidChart', sigmoid, '#0d6efd');
        createChart('reluChart', relu, '#198754');
        createChart('leakyReluChart', leakyRelu, '#dc3545');
        createChart('softmaxChart', softmax, '#ffc107');
        createChart('softplusChart', softplus, '#0dcaf0');
        createChart('swishChart', swish, '#6f42c1');
        createChart('geluChart', gelu, '#fd7e14');
    });
    </script>
</body>
</html>
